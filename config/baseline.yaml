---
# Baseline Experiment Configuration
experiment_name: "baseline_policy_comparison"
description: "Compare different game policies on standard games"

# Game settings
game:
  name: "tic_tac_toe"  # OpenSpiel game name
  parameters: {}       # Game-specific parameters

# Policies to evaluate
policies:
  - name: "random"
    parameters:
      seed: 42
    
  - name: "intuitive_gamer" 
    parameters:
      opponent_inference:
        enabled: true
        method: "log_likelihood"  # or "agreement_count"
        candidate_policies:
          - name: "random"
            parameters:
              seed: 42
          - name: "intuitive_gamer" 
            parameters: {}
          - name: "mcts"
            parameters:
              iterations: 1000         # Number of MCTS simulations per move
              exploration_weight: 1.4 # Exploration parameter for UCT
              max_depth: 50             # Maximum depth for simulations
              stochastic: false         # Whether to return deterministic or stochastic action likelihoods
        params:
          # Method-specific parameters
          log_likelihood:
            smoothing: 0.01  # Add small epsilon to avoid log(0)
          agreement_count:
            threshold: 0.5   # Minimum prob to count as "agreement"


  - name: "mcts"
    parameters:
      iterations: 1000         # Number of MCTS simulations per move
      exploration_weight: 1.4 # Exploration parameter for UCT
      max_depth: 50             # Maximum depth for simulations
      stochastic: false         # Whether to return deterministic or stochastic action likelihoods
      


sampler:
  predicates:
    - "lambda x: x['current_player'] in [-1, 1]"
    - "lambda x: x['winning'] in [True]"
    - "lambda x: x['num_turns'] <= 3"
    - "lambda x: x['longest_chain_me'] > 0"
    - "lambda x: x['longest_chain_opp'] > 0"    
  sample:
    k: 1000
    replace: false
